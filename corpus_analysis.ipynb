{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbed8a2",
   "metadata": {},
   "source": [
    "# Sovereign Triad Corpus Analysis\n",
    "\n",
    "This notebook analyzes the `triad_corpus.json` file to extract insights about the Sovereign Triad framework. We will perform the following analysis:\n",
    "1.  **Load and Inspect Data**: Load the JSON corpus and examine its structure.\n",
    "2.  **Key Term Frequency**: Count the occurrences of core concepts like \"Truth,\" \"Wisdom,\" and \"Humanity.\"\n",
    "3.  **Visualize Term Usage**: Create visualizations to see how term usage varies across different sections of the document.\n",
    "4.  **Relationship Graph**: Build and visualize a network graph of the sections to understand their relationships.\n",
    "5.  **Centrality Analysis**: Identify the most central concepts in the framework using network analysis metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58ea4e",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect the Corpus Data\n",
    "First, we load the `triad_corpus.json` file and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the corpus data\n",
    "with open('triad_corpus.json', 'r', encoding='utf-8') as f:\n",
    "    corpus_data = json.load(f)\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame(corpus_data)\n",
    "\n",
    "# Display the first few rows and the structure of the DataFrame\n",
    "print(\"Corpus Data Structure:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71897cb",
   "metadata": {},
   "source": [
    "## 2. Analyze Key Term Frequency\n",
    "Here, we'll count the occurrences of the core terms \"Truth\", \"Wisdom\", and \"Humanity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Define key terms\n",
    "key_terms = ['truth', 'wisdom', 'humanity']\n",
    "\n",
    "# Concatenate all text for a global count\n",
    "all_text = ' '.join(df['paragraph']).lower()\n",
    "\n",
    "# Find all occurrences of the key terms\n",
    "term_counts = Counter(re.findall(r'\\b(' + '|'.join(key_terms) + r')\\b', all_text))\n",
    "\n",
    "print(\"Key Term Frequencies:\")\n",
    "for term, count in term_counts.items():\n",
    "    print(f\"- {term.capitalize()}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7b252",
   "metadata": {},
   "source": [
    "## 3. Visualize Term Usage Across Sections\n",
    "Now, let's visualize how the usage of these terms varies across the main sections of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate term frequency per section\n",
    "term_freq_by_section = df.groupby('subsection')['paragraph'].apply(lambda x: ' '.join(x).lower()).apply(lambda x: pd.Series({term: len(re.findall(r'\\b' + term + r'\\b', x)) for term in key_terms}))\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(term_freq_by_section, annot=True, cmap='viridis', fmt='g')\n",
    "plt.title('Key Term Frequency by Subsection')\n",
    "plt.xlabel('Key Terms')\n",
    "plt.ylabel('Subsection')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba2420",
   "metadata": {},
   "source": [
    "## 4. Construct and Visualize the Relationship Graph\n",
    "We'll create a graph where each subsection is a node. An edge exists between two nodes if they belong to the same main section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for subsection in df['subsection'].unique():\n",
    "    G.add_node(subsection)\n",
    "\n",
    "# Add edges between subsections within the same main section\n",
    "# A simple approach: connect all subsections that share a main section title part\n",
    "df['main_section'] = df['subsection'].apply(lambda x: x.split(' > ')[0])\n",
    "for section, group in df.groupby('main_section'):\n",
    "    for u, v in combinations(group['subsection'].unique(), 2):\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(14, 14))\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, font_size=8, node_color='skyblue', edge_color='gray')\n",
    "plt.title('Relationship Graph of Subsections')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdf37e",
   "metadata": {},
   "source": [
    "## 5. Identify Central Concepts with Network Analysis\n",
    "Finally, we'll calculate centrality measures to identify the most influential subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000) # Increased max_iter\n",
    "\n",
    "# Create a DataFrame for centrality scores\n",
    "centrality_df = pd.DataFrame({\n",
    "    'Degree Centrality': pd.Series(degree_centrality),\n",
    "    'Betweenness Centrality': pd.Series(betweenness_centrality),\n",
    "    'Eigenvector Centrality': pd.Series(eigenvector_centrality)\n",
    "})\n",
    "\n",
    "# Sort by degree centrality to find the most connected concepts\n",
    "print(\"Top 5 Central Concepts by Degree Centrality:\")\n",
    "print(centrality_df.sort_values(by='Degree Centrality', ascending=False).head())\n",
    "\n",
    "print(\"\\nTop 5 Central Concepts by Betweenness Centrality:\")\n",
    "print(centrality_df.sort_values(by='Betweenness Centrality', ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
